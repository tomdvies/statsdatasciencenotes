% --- LaTeX Lecture Notes Template - S. Venkatraman ---equire"cmp.utils.feedkeys".run(378)


% --- Set document class and font size ---

\documentclass[letterpaper, 12pt]{article}

% Extended set of colors
\usepackage[dvipsnames]{xcolor}

\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont, units,          % Math typesetting
  graphicx, wrapfig, subfig, float,                            % Figures and graphics formatting
  listings, color, inconsolata, pythonhighlight,               % Code formatting
  fancyhdr, hyperref, enumerate, enumitem, framed }   % Headers/footers, section fonts, links, lists

% lipsum is just for generating placeholder text and can be removed
\usepackage{hyperref, lipsum} 

% --- Fonts ---

\usepackage{newpxtext, newpxmath, inconsolata}

% --- Page layout settings ---

% Set page margins
\usepackage[left=0.7in, right=0.7in, top=1.0in, bottom=.9in, headsep=.2in, footskip=0.35in]{geometry}
\setlength{\headheight}{13.59999pt}

% Anchor footnotes to the bottom of the page
\usepackage[bottom]{footmisc}

% Set line spacing
\renewcommand{\baselinestretch}{1.2}

% Set spacing between paragraphs
\setlength{\parskip}{1.3mm}

% Allow multi-line equations to break onto the next page
\allowdisplaybreaks

% --- Page formatting settings ---

% Set image captions to be italicized
\usepackage[font={it,footnotesize}]{caption}

% Set link colors for labeled items (blue), citations (red), URLs (orange)
\hypersetup{colorlinks=true, linkcolor=MidnightBlue, citecolor=RedOrange, urlcolor=MidnightBlue}

% Set font size for section titles (\large) and subtitles (\normalsize) 
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{{\fontsize{19}{19}\selectfont}}{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\selectfont}{\thesubsection\;\;\;}{0em}{}

% Enumerated/bulleted lists: make numbers/bullets flush left
%\setlist[enumerate]{wide=2pt, leftmargin=16pt, labelwidth=0pt}
\setlist[itemize]{wide=0pt, leftmargin=16pt, labelwidth=10pt, align=left}

% --- Table of contents settings ---

\usepackage[subfigure]{tocloft}

% add dot after number
% \renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

% Reduce spacing between sections in table of contents
\setlength{\cftbeforesecskip}{.9ex}

% Remove indentation for sections
\cftsetindents{section}{0em}{0em}

% Set font size (\large) for table of contents title
\renewcommand{\cfttoctitlefont}{\large\bfseries}

% Remove numbers/bullets from section titles in table of contents
\makeatletter
\renewcommand{\cftsecpresnum}{\begin{lrbox}{\@tempboxa}}
\renewcommand{\cftsecaftersnum}{\end{lrbox}}
\makeatother

% --- Set path for images ---

\graphicspath{{Images/}{../Images/}}

% --- Math/Statistics commands ---

% Add a reference number to a single line of a multi-line equation
% Usage: "\numberthis\label{labelNameHere}" in an align or gather environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Shortcut for bold text in math mode, e.g. $\b{X}$
\let\b\mathbf

% Shortcut for bold Greek letters, e.g. $\bg{\beta}$
\let\bg\boldsymbol

% Shortcut for calligraphic script, e.g. %\mc{M}$
\let\mc\mathcal

% \mathscr{(letter here)} is sometimes used to denote vector spaces
\usepackage[mathscr]{euscript}

% Convergence: right arrow with optional text on top
% E.g. $\converge[p]$ for converges in probability
\newcommand{\converge}[1][]{\xrightarrow{#1}}

% Weak convergence: harpoon symbol with optional text on top
% E.g. $\wconverge[n\to\infty]$
\newcommand{\wconverge}[1][]{\stackrel{#1}{\rightharpoonup}}

% Equality: equals sign with optional text on top
% E.g. $X \equals[d] Y$ for equality in distribution
\newcommand{\equals}[1][]{\stackrel{\smash{#1}}{=}}

% Normal distribution: arguments are the mean and variance
% E.g. $\normal{\mu}{\sigma}$
\newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}

% Uniform distribution: arguments are the left and right endpoints
% E.g. $\unif{0}{1}$
\newcommand{\unif}[2]{\text{Uniform}(#1,#2)}

% Independent and identically distributed random variables
% E.g. $ X_1,...,X_n \iid \normal{0}{1}$
\newcommand{\iid}{\stackrel{\smash{\text{iid}}}{\sim}}

% Sequences (this shortcut is mostly to reduce finger strain for small hands)
% E.g. to write $\{A_n\}_{n\geq 1}$, do $\bk{A_n}{n\geq 1}$
\newcommand{\bk}[2]{\{#1\}_{#2}}

% Math mode symbols for common sets and spaces. Example usage: $\R$
\newcommand{\R}{\mathbb{R}}	% Real numbers
\newcommand{\C}{\mathbb{C}}	% Complex numbers
\newcommand{\Q}{\mathbb{Q}}	% Rational numbers
\newcommand{\Z}{\mathbb{Z}}	% Integers
\newcommand{\N}{\mathbb{N}}	% Natural numbers
\newcommand{\F}{\mathcal{F}}	% Calligraphic F for a sigma algebra
\newcommand{\El}{\mathcal{L}}	% Calligraphic L, e.g. for L^p spaces

% Math mode symbols for probability
\newcommand{\pr}{\mathbb{P}}	% Probability measure
\newcommand{\E}{\mathbb{E}}	% Expectation, e.g. $\E(X)$
\newcommand{\var}{\text{Var}}	% Variance, e.g. $\var(X)$
\newcommand{\cov}{\text{Cov}}	% Covariance, e.g. $\cov(X,Y)$
\newcommand{\corr}{\text{Corr}}	% Correlation, e.g. $\corr(X,Y)$
\newcommand{\B}{\mathcal{B}}	% Borel sigma-algebra

% Other miscellaneous symbols
\newcommand{\tth}{\text{th}}	% Non-italicized 'th', e.g. $n^\tth$
\newcommand{\Oh}{\mathcal{O}}	% Big-O notation, e.g. $\O(n)$
\newcommand{\1}{\mathds{1}}	% Indicator function, e.g. $\1_A$

% Additional commands for math mode
\DeclareMathOperator*{\argmax}{argmax}		% Argmax, e.g. $\argmax_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\argmin}{argmin}		% Argmin, e.g. $\argmin_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\spann}{Span}		% Span, e.g. $\spann\{X_1,...,X_n\}$
\DeclareMathOperator*{\bias}{Bias}		% Bias, e.g. $\bias(\hat\theta)$
\DeclareMathOperator*{\ran}{ran}			% Range of an operator, e.g. $\ran(T) 
\DeclareMathOperator*{\dv}{d\!}			% Non-italicized 'with respect to', e.g. $\int f(x) \dv x$
\DeclareMathOperator*{\diag}{diag}		% Diagonal of a matrix, e.g. $\diag(M)$
\DeclareMathOperator*{\trace}{trace}		% Trace of a matrix, e.g. $\trace(M)$
\DeclareMathOperator*{\supp}{supp}		% Support of a function, e.g., $\supp(f)$

% Numbered theorem, lemma, etc. settings - e.g., a definition, lemma, and theorem appearing in that 
% order in Lecture 2 will be numbered Definition 2.1, Lemma 2.2, Theorem 2.3. 
% Example usage: \begin{theorem}[Name of theorem] Theorem statement \end{theorem}
\newtheoremstyle{note}% name
{6pt}% Space above
{0pt}% Space below
{}% Body font
{}% Indent amount
{\bfseries}% Theorem head font
{:}% Punctuation after theorem head
{.5em}% Space after theorem head
{}% Theorem head spec (can be left empty, meaning ‘normal’)

% \theoremstyle{note}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{example}[theorem]{Example}
% \newtheorem{remark}[theorem]{Remark}

% Un-numbered theorem, lemma, etc. settings
% Example usage: \begin{lemma*}[Name of lemma] Lemma statement \end{lemma*}
% \newtheorem*{theorem*}{Theorem}
% \newtheorem*{proposition*}{Proposition}
% \newtheorem*{lemma*}{Lemma}
% \newtheorem*{corollary*}{Corollary}
% \newtheorem*{definition*}{Definition}
% \newtheorem*{example*}{Example}
% \newtheorem*{remark*}{Remark}
% \newtheorem*{claim}{Claim}

\usepackage[most]{tcolorbox}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\tcolorboxenvironment{definition}{enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners}
\tcolorboxenvironment{theorem}{enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners}

% --- Left/right header text (to appear on every page) ---

% Do not include a line under header or above footer
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\setcounter{section}{-1}
% Right header text: Lecture number and title
\renewcommand{\sectionmark}[1]{\markright{#1} }
\fancyhead[R]{\small\textit{\nouppercase{\rightmark}}}

% Left header text: Short course title, hyperlinked to table of contents
\fancyhead[L]{\small\textit{2024 MLPR Notes}}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\usepackage[parfill]{parskip}
\usepackage{lipsum}
% --- Document starts here ---
\setlength\abovedisplayskip{5pt plus 2pt minus 2pt}
\begin{document}

% --- Main title and subtitle ---

\title{Machine Learning and Pattern Recognition \\[1em]
\normalsize Notes from lectures given by Dr Arno Onken in 2024}

% --- Author and date of last update ---

\author{\normalsize Thomas Davies}
\date{\normalsize\vspace{-1ex} Last updated: \today}

% --- Add title and table of contents ---

\maketitle
\tableofcontents\label{sec:contents}
% --- Main content: import lectures as subfiles ---

%\input{Lectures/Lecture1}

% --- Bibliography ---

% Start a bibliography with one item.
% Citation example: "\cite{williams}".
\newpage
\section{Introduction}
These are notes that were taken in 2024 for the \href{https://mlpr.inf.ed.ac.uk/2024/}{MLPR} course with lectures given by \href{https://homepages.inf.ed.ac.uk/aonken/}{Dr Arno Onken}. 
They were mostly taken live in lectures, in particular, all errors are almost surely mine.
The provided lecture notes can be found \href{https://mlpr.inf.ed.ac.uk/2024/notes/}{here}. These are not endorsed by the lecturer, and are mostly for my own reference.
It should be noted that these notes will be much more maths focused than the ones provided in lectures, this is mainly for my own comfort (I come from a maths background).
I may also later add a GitHub link with python (or maybe rust) implementations of the methods provided from scratch (where scratch is defined as starting from some base linear algebra library like numpy).

\section{Lecture 1}
\label{sec:Lecture 1}
This lecture was primarily describing why one would want to study machine learning, and gave some interesting examples such as \href{https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807}{face detection}. I would recommend reading the provided \href{https://mlpr.inf.ed.ac.uk/2024/notes/w1a_intro.html}{notes}, however I don't think there is any value in me reproducing this text.

\section{Lecture 2}
\label{sec:Lecture 2}
Very very often, machine learning problems can be reduced in some form to linear regression (see \href{https://en.wikipedia.org/wiki/Kernel_method}{the Kernel Method}, \href{https://en.wikipedia.org/wiki/Kriging}{Gaussian Process Regression}, and more exotically the \href{https://arxiv.org/pdf/1811.03962}{Kernel Regime} for neural networks). 
So it is important to understand the basics of what we are doing in linear regression. First we need a couple of basic definitions.

\begin{definition}
    \label{def:affine function}
    An \textbf{affine function} \(f: \R^k \to \R^m\) is one of the form
    \[
        f_{A,b}(x) = Ax + b
    \]
    where \(x\in\R^k\), \(A\in\R^{m\times k}\), and \(b \in \R^m\). Note that notationally \(m,k\) are often renamed and can change based on the context (although it is normally easy to infer the dimensions).
\end{definition}

In the lectures, \(m\) was taken to be \(1\), and \(A\) was denoted as \(w^T\), and none of the derivations of the ensuing results were given, however I think there is value in doing them. We now also assume that we have some \textbf{training data}.
\begin{definition}
    \textbf{Supervised training data} is a pair of "labelled" data
    \[
        \mathscr{T} = \{(x^{(n)}, y^{(n)}) \mid n=1,2,\ldots,N\}
    \]
    In the unsupervised case (which we cover later in the course), we are missing \(y^{(n)}\) but still hope to be able to recover our underlying function via building structure into our model.
\end{definition}
We assume that it was sampled from a \textbf{linear model} i.e. there is some approximate relationship \(f(x_n) \approx y_n\) for some affine function \(f\) (for a slightly better way to describe this see \href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{ordinary least squares}). We would like a way to recover a good estimate of \(A\) and \(b\) from out training data, one way to do this is to minimise the loss in the \(2\)-norm (squared error). We will denote our approximated (affine) function as \(\tilde{f}\), and our approximated parameters as \(\tilde{A}\), and \(\tilde{b}\). The loss function will be denoted as \(\mathscr{L}(g)\). Using the \(2\)-norm (with no regularisation).
\begin{equation}
    \label{eqn: lsq loss}
    \mathscr{L}(\tilde{f}) = \sum_{n=1}^N \norm{y^{(n)} - \tilde{f}(x^{(n)})}^2_2 = \sum_{n=1}^N \sum_{i=1}^k (y^{(n)}_i - \tilde{f}_i(x^{(n)}))^2
\end{equation}
We seek (as we normally do in machine learning) to minimise this loss function over our parameters \(\tilde{A}\), \(\tilde{b}\).
Note that as a sanity check, if we perfectly recovered \(f\), that the loss would be \(0\) i.e. \(\mathscr{L}(f) = 0\). 
Now, let's try find a closed form solution (under certain regularity conditions which we will state as they arise). 
\begin{definition}
    \label{def:design matrix}
    A \textbf{design matrix} \(X \in \R^{N\times k}\), and \textbf{output matrix} \(Y\in \R^{N \times m}\) are stacked versions of our training data.
    \begin{equation*}
        X = 
        \begin{bmatrix}
            -&x^{(1)T} &-\\ -&x^{(2)T}&- \\ &\vdots& \\ -&x^{(N)T}&-
        \end{bmatrix} \hspace{45pt}
        Y = 
        \begin{bmatrix}
            -&y^{(1)T}&- \\ - &y^{(2)T}&- \\ & \vdots&  \\ - & y^{(N)T} & -
        \end{bmatrix}
    \end{equation*}
\end{definition}
We can re write the loss in equation \ref{eqn: lsq loss} by subbing in our parameters, and setting \(\tilde{b}=0\) (this will be expounded on later) as follows.
\begin{equation}
    \mathscr{L}(\tilde{f}) = \sum_{n=1}^N (y^{(n)} - \tilde{A}x^{(n)})^T(y^{(n)} - \tilde{A}x^{(n)})
\end{equation}
Now, differentiating with respect to \(\tilde{A}\), we get the following.
\begin{equation}
    \frac{\delta \mathscr{L}}{\partial \tilde{A}}(\tilde{f}) = 2\sum_{n=1}^N (y^{(n)} - \tilde{A}x^{(n)})
\end{equation}
Setting the derivative to 0 (it is clearly a global minima as when \(\tilde{A} \to \infty\) we have \(\mathscr{L}(\tilde{f}) \to \infty\) under the regularity condition stated below), and rearranging, we get the closed form solution.
\begin{equation}
    \tilde{A} = 
\end{equation}
% \newpage
% \begin{thebibliography}{1}
%
% \bibitem{williams}
%    Williams, David.
%    \textit{Probability with Martingales}.
%    Cambridge University Press, 1991.
%    Print.
%
% % Uncomment the following lines to include a webpage
% % \bibitem{webpage1}
% %   LastName, FirstName. ``Webpage Title''.
% %   WebsiteName, OrganizationName.
% %   Online; accessed Month Date, Year.\\
% %   \texttt{www.URLhere.com}
%
% \end{thebibliography}
% --- Document ends here ---

\end{document}
