% --- LaTeX Lecture Notes Template - S. Venkatraman ---equire"cmp.utils.feedkeys".run(378)


% --- Set document class and font size ---

\documentclass[letterpaper, 12pt]{article}

% Extended set of colors
\usepackage[dvipsnames]{xcolor}

\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont, units,          % Math typesetting
  graphicx, wrapfig, subfig, float,                            % Figures and graphics formatting
  listings, color, inconsolata, pythonhighlight,               % Code formatting
  fancyhdr, hyperref, enumerate, enumitem, framed }   % Headers/footers, section fonts, links, lists

% lipsum is just for generating placeholder text and can be removed
\usepackage{hyperref, lipsum} 

% --- Fonts ---

\usepackage{newpxtext, newpxmath, inconsolata}

% --- Page layout settings ---

% Set page margins
\usepackage[left=0.7in, right=0.7in, top=1.0in, bottom=.9in, headsep=.2in, footskip=0.35in]{geometry}
\setlength{\headheight}{13.59999pt}

% Anchor footnotes to the bottom of the page
\usepackage[bottom]{footmisc}

% Set line spacing
\renewcommand{\baselinestretch}{1.2}

% Set spacing between paragraphs
\setlength{\parskip}{1.3mm}

% Allow multi-line equations to break onto the next page
\allowdisplaybreaks

% --- Page formatting settings ---

% Set image captions to be italicized
\usepackage[font={it,footnotesize}]{caption}

% Set link colors for labeled items (blue), citations (red), URLs (orange)
\hypersetup{colorlinks=true, linkcolor=MidnightBlue, citecolor=RedOrange, urlcolor=MidnightBlue}

% Set font size for section titles (\large) and subtitles (\normalsize) 
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{{\fontsize{19}{19}\selectfont}}{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\selectfont}{\thesubsection\;\;\;}{0em}{}

% Enumerated/bulleted lists: make numbers/bullets flush left
%\setlist[enumerate]{wide=2pt, leftmargin=16pt, labelwidth=0pt}
\setlist[itemize]{wide=0pt, leftmargin=16pt, labelwidth=10pt, align=left}

% fig insert

\def\figinsert#1#2#3{
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{#1}
    \caption{#2}
    \label{#3}
    \end{figure}%
}

% --- Table of contents settings ---

\usepackage[subfigure]{tocloft}

% add dot after number
% \renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\setcounter{tocdepth}{2}
% Reduce spacing between sections in table of contents
\setlength{\cftbeforesecskip}{.9ex}

% Remove indentation for sections
\cftsetindents{section}{0em}{0em}

% Set font size (\large) for table of contents title
\renewcommand{\cfttoctitlefont}{\large\bfseries}

% Remove numbers/bullets from section titles in table of contents
\makeatletter
\renewcommand{\cftsecpresnum}{\begin{lrbox}{\@tempboxa}}
\renewcommand{\cftsecaftersnum}{\end{lrbox}}
\makeatother

% --- Set path for images ---

\graphicspath{{Images/}{../Images/}}

% --- Math/Statistics commands ---

% Add a reference number to a single line of a multi-line equation
% Usage: "\numberthis\label{labelNameHere}" in an align or gather environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Shortcut for bold text in math mode, e.g. $\b{X}$
\let\b\mathbf

% Shortcut for bold Greek letters, e.g. $\bg{\beta}$
\let\bg\boldsymbol

% Shortcut for calligraphic script, e.g. %\mc{M}$
\let\mc\mathcal

% \mathscr{(letter here)} is sometimes used to denote vector spaces
\usepackage[mathscr]{euscript}

% Convergence: right arrow with optional text on top
% E.g. $\converge[p]$ for converges in probability
\newcommand{\converge}[1][]{\xrightarrow{#1}}

% Weak convergence: harpoon symbol with optional text on top
% E.g. $\wconverge[n\to\infty]$
\newcommand{\wconverge}[1][]{\stackrel{#1}{\rightharpoonup}}

% Equality: equals sign with optional text on top
% E.g. $X \equals[d] Y$ for equality in distribution
\newcommand{\equals}[1][]{\stackrel{\smash{#1}}{=}}

% Normal distribution: arguments are the mean and variance
% E.g. $\normal{\mu}{\sigma}$
\newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}

% Uniform distribution: arguments are the left and right endpoints
% E.g. $\unif{0}{1}$
\newcommand{\unif}[2]{\text{Uniform}(#1,#2)}

% Independent and identically distributed random variables
% E.g. $ X_1,...,X_n \iid \normal{0}{1}$
\newcommand{\iid}{\stackrel{\smash{\text{iid}}}{\sim}}

% Sequences (this shortcut is mostly to reduce finger strain for small hands)
% E.g. to write $\{A_n\}_{n\geq 1}$, do $\bk{A_n}{n\geq 1}$
\newcommand{\bk}[2]{\{#1\}_{#2}}

% Math mode symbols for common sets and spaces. Example usage: $\R$
\newcommand{\R}{\mathbb{R}}	% Real numbers
\newcommand{\C}{\mathbb{C}}	% Complex numbers
\newcommand{\Q}{\mathbb{Q}}	% Rational numbers
\newcommand{\Z}{\mathbb{Z}}	% Integers
\newcommand{\N}{\mathbb{N}}	% Natural numbers
\newcommand{\F}{\mathcal{F}}	% Calligraphic F for a sigma algebra
\newcommand{\El}{\mathcal{L}}	% Calligraphic L, e.g. for L^p spaces

% Math mode symbols for probability
\newcommand{\pr}{\mathbb{P}}	% Probability measure
\newcommand{\E}{\mathbb{E}}	% Expectation, e.g. $\E(X)$
\newcommand{\var}{\text{Var}}	% Variance, e.g. $\var(X)$
\newcommand{\cov}{\text{Cov}}	% Covariance, e.g. $\cov(X,Y)$
\newcommand{\corr}{\text{Corr}}	% Correlation, e.g. $\corr(X,Y)$
\newcommand{\B}{\mathcal{B}}	% Borel sigma-algebra

% Other miscellaneous symbols
\newcommand{\tth}{\text{th}}	% Non-italicized 'th', e.g. $n^\tth$
\newcommand{\Oh}{\mathcal{O}}	% Big-O notation, e.g. $\O(n)$
\newcommand{\1}{\mathds{1}}	% Indicator function, e.g. $\1_A$

% Additional commands for math mode
\DeclareMathOperator*{\argmax}{argmax}		% Argmax, e.g. $\argmax_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\argmin}{argmin}		% Argmin, e.g. $\argmin_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\spann}{Span}		% Span, e.g. $\spann\{X_1,...,X_n\}$
\DeclareMathOperator*{\bias}{Bias}		% Bias, e.g. $\bias(\hat\theta)$
\DeclareMathOperator*{\ran}{ran}			% Range of an operator, e.g. $\ran(T) 
\DeclareMathOperator*{\dv}{d\!}			% Non-italicized 'with respect to', e.g. $\int f(x) \dv x$
\DeclareMathOperator*{\diag}{diag}		% Diagonal of a matrix, e.g. $\diag(M)$
\DeclareMathOperator*{\trace}{trace}		% Trace of a matrix, e.g. $\trace(M)$
\DeclareMathOperator*{\supp}{supp}		% Support of a function, e.g., $\supp(f)$

% Numbered theorem, lemma, etc. settings - e.g., a definition, lemma, and theorem appearing in that 
% order in Lecture 2 will be numbered Definition 2.1, Lemma 2.2, Theorem 2.3. 
% Example usage: \begin{theorem}[Name of theorem] Theorem statement \end{theorem}
\newtheoremstyle{note}% name
{6pt}% Space above
{0pt}% Space below
{}% Body font
{}% Indent amount
{\bfseries}% Theorem head font
{:}% Punctuation after theorem head
{.5em}% Space after theorem head
{}% Theorem head spec (can be left empty, meaning ‘normal’)

% \theoremstyle{note}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{example}[theorem]{Example}
% \newtheorem{remark}[theorem]{Remark}

% Un-numbered theorem, lemma, etc. settings
% Example usage: \begin{lemma*}[Name of lemma] Lemma statement \end{lemma*}
% \newtheorem*{theorem*}{Theorem}
% \newtheorem*{proposition*}{Proposition}
% \newtheorem*{lemma*}{Lemma}
% \newtheorem*{corollary*}{Corollary}
% \newtheorem*{definition*}{Definition}
% \newtheorem*{example*}{Example}
% \newtheorem*{remark*}{Remark}
% \newtheorem*{claim}{Claim}

\usepackage[most]{tcolorbox}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\tcolorboxenvironment{definition}{enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners}
\tcolorboxenvironment{theorem}{enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners}

% --- Left/right header text (to appear on every page) ---

% Do not include a line under header or above footer
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\setcounter{section}{-1}
% Right header text: Lecture number and title
\renewcommand{\sectionmark}[1]{\markright{#1} }
\fancyhead[R]{\small\textit{\nouppercase{\rightmark}}}

% Left header text: Short course title, hyperlinked to table of contents
\fancyhead[L]{\small\textit{2024 MLPR Notes}}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\usepackage[parfill]{parskip}
\usepackage{lipsum}
% --- Document starts here ---
\setlength\abovedisplayskip{5pt plus 2pt minus 2pt}

% Code higlighting

\usepackage{minted}
\usemintedstyle{vs}
\usepackage{etoolbox}
\BeforeBeginEnvironment{minted}%
{\begin{tcolorbox}[enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners]}%
\AfterEndEnvironment{minted}
   {\end{tcolorbox}}%

%\BeforeBeginEnvironment{figure}%
%{\begin{tcolorbox}[enhanced jigsaw,colback={white}, colframe=black,boxrule=1.3pt, sharp corners]
%    \begin{center}}%
%\AfterEndEnvironment{figure}
%{\end{center}
%\end{tcolorbox}}%

\begin{document}

% --- Main title and subtitle ---

\title{Machine Learning and Pattern Recognition \\[1em]
\normalsize Notes from lectures given by Dr Arno Onken in 2024}

% --- Author and date of last update ---

\author{\normalsize Thomas Davies}
\date{\normalsize\vspace{-1ex} Last updated: \today}

% --- Add title and table of contents ---

\maketitle
\tableofcontents\label{sec:contents}
% --- Main content: import lectures as subfiles ---

%\input{Lectures/Lecture1}

% --- Bibliography ---

% Start a bibliography with one item.
% Citation example: "\cite{williams}".
\newpage
\section{Introduction}
These are notes that were taken in 2024 for the \href{https://mlpr.inf.ed.ac.uk/2024/}{MLPR} course with lectures given by \href{https://homepages.inf.ed.ac.uk/aonken/}{Dr Arno Onken}. 
They were mostly taken live in lectures, in particular, all errors are almost surely mine.
The provided lecture notes can be found \href{https://mlpr.inf.ed.ac.uk/2024/notes/}{here}. These are not endorsed by the lecturer, and are mostly for my own reference.
It should be noted that these notes will be much more maths focused than the ones provided in lectures, this is mainly for my own comfort (I come from a maths background).
I may also later add a GitHub link with python (or maybe rust) implementations of the methods provided from scratch (where scratch is defined as starting from some base linear algebra library like numpy).

\section{Lecture 1}
\label{sec:Lecture 1}
This lecture was primarily describing why one would want to study machine learning, and gave some interesting examples such as \href{https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807}{face detection}. I would recommend reading the provided \href{https://mlpr.inf.ed.ac.uk/2024/notes/w1a_intro.html}{notes}, however I don't think there is any value in me reproducing this text.

\section{Lecture 2}
\label{sec:Lecture 2}
\subsection{Least Squares}
Very very often, machine learning problems can be reduced in some form to linear regression (see \href{https://en.wikipedia.org/wiki/Kernel_method}{the Kernel Method}, \href{https://en.wikipedia.org/wiki/Kriging}{Gaussian Process Regression}, and more exotically the \href{https://arxiv.org/pdf/1811.03962}{Kernel Regime} for neural networks). 
So it is important to understand the basics of what we are doing in linear regression. First we need a couple of basic definitions.

\begin{definition}
    \label{def:affine function}
    An \textbf{affine function} \(f: \R^k \to \R^m\) is one of the form
    \[
        f_{A,b}(x) = Ax + b
    \]
    where \(x\in\R^k\), \(A\in\R^{m\times k}\), and \(b \in \R^m\). Note that notationally \(m,k\) are often renamed and can change based on the context (although it is normally easy to infer the dimensions).
\end{definition}

In the lectures, \(m\) was taken to be \(1\), and \(A\) was denoted as \(w^T\), and none of the derivations of the ensuing results were given, however I think there is value in doing them. We now also assume that we have some \textbf{training data}.
\begin{definition}
    \textbf{Supervised training data} is a pair of "labelled" data
    \[
        \mathscr{T} = \{(x^{(n)}, y^{(n)}) \mid n=1,2,\ldots,N\}
    \]
    In the unsupervised case (which we cover later in the course), we are missing \(y^{(n)}\) but still hope to be able to recover our underlying function via building structure into our model.
\end{definition}
We assume that it was sampled from a \textbf{linear model} i.e. there is some approximate relationship \(f(x_n) \approx y_n\) for some affine function \(f\) (for a slightly better way to describe this see \href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{ordinary least squares}). We would like a way to recover a good estimate of \(A\) and \(b\) from out training data, one way to do this is to minimise the loss in the \(2\)-norm (squared error). We will denote our approximated (affine) function as \(\tilde{f}\), and our approximated parameters as \(\tilde{A}\), and \(\tilde{b}\). The loss function will be denoted as \(\mathscr{L}(g)\). Using the \(2\)-norm (with no regularisation).
\begin{equation}
    \label{eqn: lsq loss}
    \mathscr{L}(\tilde{f}) = \sum_{n=1}^N \norm{y^{(n)} - \tilde{f}(x^{(n)})}^2_2 = \sum_{n=1}^N \sum_{i=1}^k (y^{(n)}_i - \tilde{f}_i(x^{(n)}))^2
\end{equation}
We seek (as we normally do in machine learning) to minimise this loss function over our parameters \(\tilde{A}\), \(\tilde{b}\).
Note that as a sanity check, if we perfectly recovered \(f\), that the loss would be \(0\) i.e. \(\mathscr{L}(f) = 0\). 
Now, let's try find a closed form solution (under certain regularity conditions which we will state as they arise). 
\begin{definition}
    \label{def:design matrix}
    A \textbf{design matrix} \(X \in \R^{N\times k}\), and \textbf{output matrix} \(Y\in \R^{N \times m}\) are stacked versions of our training data.
    \begin{equation*}
        X = 
        \begin{bmatrix}
            -&x^{(1)T} &-\\ -&x^{(2)T}&- \\ &\vdots& \\ -&x^{(N)T}&-
        \end{bmatrix} \hspace{45pt}
        Y = 
        \begin{bmatrix}
            -&y^{(1)T}&- \\ - &y^{(2)T}&- \\ & \vdots&  \\ - & y^{(N)T} & -
        \end{bmatrix}
    \end{equation*}
\end{definition}
We can re write the loss in equation \ref{eqn: lsq loss} by subbing in our parameters, setting \(\tilde{b}=0\) (this will be expounded on later), and re-writing the norm in terms of the design matrix as follows, where \(\tilde{a}_i\) is the \(i\)th row of \(\tilde{A}\).
\begin{equation}
    \mathscr{L}(\tilde{f}) = \sum_{i=1}^m (y^{(n)}_i - \tilde{a}_i X)^T(y^{(n)}_i - \tilde{a}_i X)
\end{equation}
Now, differentiating with respect to \(\tilde{A}\), and setting it to 0 we get the following closed form for \(\tilde{A}\).
\begin{equation}
    \tilde{A} = (X^T X)^{-1}X^T Y
\end{equation}
Where we require \(X^T X\) to be invertible for this to hold. This is clearly a global minima as when \(\tilde{A} \to \infty\) we have \(\mathscr{L}(\tilde{f}) \to \infty\) under the condition that \(X^TX\) is non singular). If this is non obvious, note that we can minimise each row individually. 

As for why we can wlog \(b=0\), we simply extend our matrix by one collum, and add one synthetic "observation" collum to Y with values all 1, one synthetic "input" collum to X with all 0s. So our design, output matrices become.
\begin{equation}
    X = 
    \begin{bmatrix}
        -&x^{(1)T} &-&1\\ -&x^{(2)T}&-&1 \\ &\vdots& \\ -&x^{(N)T}&- & 1
    \end{bmatrix} \hspace{30pt}
    Y = 
    \begin{bmatrix}
        -&y^{(1)T}&-&0 \\ - &y^{(2)T}&-&0 \\ & \vdots&  \\ - & y^{(N)T} & - & 0
    \end{bmatrix} \hspace{30pt}
    \tilde{A} =
    \begin{bmatrix}
        -&\tilde{a}^{(1)T} &-&\tilde{b}_1\\ -&\tilde{a}^{(2)T}&-& \tilde{b}_2\\ &\vdots& \\ -&\tilde{a}^{(m)T}&- & \tilde{b}_m
    \end{bmatrix}
\end{equation}
The resulting loss function is the same, so all of our results above still hold. 

Let's take some data, and find the least squares fit of it in python, we will use the \href{https://numpy.org/}{numpy} library, which has a lstsq function builtin.
\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt

x_grid = np.arange(-3, 3, 1)
m = 5
variance = 2
f_vals = m*x_grid + np.random.normal(0, variance, len(x_grid))
plt.plot(x_grid, m*x_grid, 'b-')
plt.plot(x_grid, f_vals, 'r.')
mpred = np.linalg.lstsq(np.array([x_grid]).T, f_vals, rcond=None)[0]
plt.plot(x_grid, mpred*x_grid, 'g-')
plt.show()
\end{minted}
We have setup a 1 dimensional linear regression problem, with normal noise and our gradient being \(5\). Running the code, we predict that the gradient is \(4.508\), which is quite close to the true value. Plotting them in figure \ref{fig: lsqplot}, we can see how even though the sampled points are quite far off the line, we can still get a good estimate when reconstructing the underlying function.
\figinsert{python/lecture2/lsqfit.eps}{Least squares fit}{fig: lsqplot}

\subsection{Linear Regression with Basis Functions}
Now, an obvious question is what if we want to fit a model which isn't linear? Well we can actually still do linear regression, but we just transform our domain prior to regressing i.e. our features go from \((x_1, x_2, \ldots, x_m)\) to \((\phi_1(x), \phi_2(x),\ldots, \phi_D(x))\) where \(D\) is the number of basis functions, and \(\phi_i : \R^m \to \R\), are some functions that we can specify. Again, if we look at the loss function above, our closed form still holds, we have just relabelled our input features to be some mapping of our vector space. This also extends to any linear regression technique like \textbf{lasso}, and \textbf{ridge} regression which we will cover later. We will now list some common basis functions.

\subsubsection{Polynomials}
An obvious first choice would be polynomials, they are dense in \(C(K)\) where \(K\subset \R^p\) is compact, so with infinite data, we could recover the function to arbitrary accuracy. However, this scales very badly in dimension as to fit a polynomial with \(D\) input features, a \(k-1\) degree polynomial needs \(\frac{(k+D-1)!}{k!(D-1)!}\) coefficients. For small dimensions however, these can be a good option. They also suffer from heavy overfitting, (which can be motivated by looking at taylor expansion remainders probably?). I will assume that you don't need me to plot some polynomials to know what they look like.

\subsubsection{Radial Basis Functions}
\begin{definition}
    A \textbf{Radial Basis Function} or \textbf{RBF} for short is a function \(\phi_{c,h^2}:\R^p \to \R\) which has the form.
    \begin{equation*}
        \phi_{c,h^2} = \exp\left(-\frac{(x-c)^T (x-c)}{h^2}\right)
    \end{equation*}
    where \(c\in\R^p\) and \(h^2 \in \R^{+}\)
\end{definition}
A couple plots of RBFs with varied values of \(c\), and \(h^2\) can be found in figure \ref{fig: rbf plot}. Note that \(c\) gives the center of the curve, and \(h^2\) is the "sharpness", where smaller values give a sharper curve, and larger ones give a wider curve.
\figinsert{python/lecture2/rbf/rbfplot.eps}{Plot of RBFs}{fig: rbf plot}

\begin{definition}
    A \textbf{Sigmoidal Function} is a function \(\psi_{v,b}:\R^p \to \R\) which has the form.
    \begin{equation*}
        \psi_{v,b} = \sigma(v^Tx + b )
    \end{equation*}
    Where \(v\in\R^p\), and \(b\in\R\), and with \(\sigma :\R \to \R\) the \textbf{Logistic Sigmoid} function, which has the following form.
    \begin{equation*}
        \sigma(r) = \frac{1}{1+\exp(-x)}
    \end{equation*}
\end{definition}
A couple of plots of sigmoidal functions with varied values of \(v\) and \(b\) can be found in figure \ref{fig: sigmoid plot}. Note that \(v\) gives the "steepness" of the curve with larger values making it steeper, and \(b\) changes the "center" of the curve. These are especially useful in \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression}.
\figinsert{python/lecture2/sigmoid/sigmoidplot.eps}{Plot of Sigmoidal functions}{fig: sigmoid plot}
\section{Lecture 3}
\subsection{Plotting and Understanding Basis Functions}
Lets try fit some sample data with various different selection of basis functions. It's important to note that \textbf{none of the choices are wrong}, regression is an inherently ill posed problem. We'll use 3 different sets of basis functions, a linear model, RBFs, and sigmoids. The code can be found below, and the plot can be found in figure \ref{fig: basisfit}.
\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt

# Set up and plot the dataset
yy = np.array([1.1, 2.3, 2.9]) # N,
X = np.array([[0.8], [1.9], [3.1]]) # N,1
# phi-functions to create various matrices of new features
# from an original matrix of 1D inputs.
def phi_linear(Xin):
    return np.hstack([np.ones((Xin.shape[0],1)), Xin])
def phi_quadratic(Xin):
    return np.hstack([np.ones((Xin.shape[0],1)), Xin, Xin**2])
def fw_rbf(xx, cc):
    """fixed-width RBF in 1d"""
    return np.exp(-(xx-cc)**2 / 2.0)
def phi_rbf(Xin):
    return np.hstack([fw_rbf(Xin, 1), fw_rbf(Xin, 2), fw_rbf(Xin, 3)])

def fit_and_plot(phi_fn, X, yy):
    # phi_fn takes N, inputs and returns N,K basis function values
    w_fit = np.linalg.lstsq(phi_fn(X), yy, rcond=None)[0] # K,
    X_grid = np.arange(0, 4, 0.01)[:,None] # N,1
    f_grid = np.dot(phi_fn(X_grid), w_fit)
    plt.plot(X_grid, f_grid)

fit_and_plot(phi_linear, X, yy)
fit_and_plot(phi_quadratic, X, yy)
fit_and_plot(phi_rbf, X, yy)
plt.plot(X, yy, 'o')
plt.legend(('linear fit', 'quadratic fit', 'rbf fit', 'data'), framealpha=1)
plt.xlabel('$x$')
plt.ylabel('$f$')
plt.show()
\end{minted}
The reason none of these are wrong is because they are all (with the slight exception of the linear model) consistent with our training data. We need to test the fits to ascertain which model is best.
\figinsert{python/lecture3/basisfit.eps}{Fits with various different basis function}{fig: basisfit}
\subsection{Overfitting, Underfitting, and Regularisation}

\newpage


% answer to q on hypothesis
% I think (please someone correct me if I'm wrong), but you can motivate this by minimising the mean squared error (which I will denote via \(\mathscr{L}(\textbf{w})\)) to make clear it's a function of \(\textbf{w}\)
% \[
%     \text{MSE} = \mathscr{L}(\textbf{w}) := \mathbb{E}\left[\sum_{n=1}^N (y^{(n)} - \textbf{w}^T\textbf{x}^{(n)})^2\right]
% \]
% We could just take derivatives here, equate to 0 and bash a bunch of algebra, but it's far less work to spot that the initial distribution given is invariant under re-labelling the indices of \(x^{(n)}\), e.g. swapping \(x^{(n)}_1\), and \(x^{(n)}_2\) for all \(n=1,2,\ldots, N\). Importantly, we can note that swapping these must also just swap the indices of \(\textbf{w}\), but our distribution was the same (as all the \(x_d^{(n)}\) are i.i.d. given \(\mu^{(n)}\)). So from this we can see that we must have the form \(\textbf{w} = (w,w,\ldots,w)\). Now we can re-write the MSE as just a function of \(w\) (the scalar not the vector).
% \[
%     \mathscr{L}(w) := \mathbb{E}\left[\sum_{n=1}^N (y^{(n)} - w \sum_{d=1}^D x^{(n)}_d)^2\right]
% \]
% Also, to make our lives easier the sum of \(x\) terms can be re-written by linearity of the normal dist (normal + normal is normal). So we introduce \(r^{(n)}\) as follows - note it is still independent of \(y^{(n)}\).
% \[
%     r^{(n)} = \sum_{d=1}^D x^{(n)}_d \sim \mathcal{N} (D \mu^{(n)}, 0.01^2 D)
% \]
% Now, subbing this in and taking the derivative of the MSE with respect to the scalar we get
% \[
%     \frac{\partial \mathscr{L}}{\partial w} = 2 \sum_{n=1}^N\mathbb{E}\left[y^{(n)} - w r^{(n)}\right] 
%     = 2 \sum_{n=1}^N\mathbb{E}\left[\mu^{(n)}(1 - Dw)\right]
% \]
% Which is clearly minimised at \(w = \frac{1}{D}\), and we're done. It is probably worth also pointing out that this isn't showing that the expectation of \(\textbf{w}\) when computed via least squares is \((\frac{1}{D},\ldots)\), but that it is the solution to the "average problem". The re-labelling technique still works in that case, i.e. you know \(\mathbb{E}[\textbf{w}] = (w,w,\ldots,w)\), but I think computing w here would be a decent amount messier. Another nice plus is that this result doesn't care about the underlying distribution of \(\mu\) (although mean 0 could give you some degeneracy issues).
%


% Now, subbing this in and taking the derivative of the MSE with respect to the scalar we get
% \[
%     \frac{\partial \mathscr{L}}{\partial w} = 2 \sum_{n=1}^N\mathbb{E}\left[(y^{(n)} - w r^{(n)})^2\right] 
%     = 2 \sum_{n=1}^N\mathbb{E}\left[(y^{(n)})^2 - 2 w y^{(n)} r^{(n)} + w^2 (r^{(n)})^2\right]
% \]
% We can now equate the derivative to 0, and take expectations conditional on \(\mu^{(n)}\).
% \[
%     0 = 2 \sum_{n=1}^N\mathbb{E}\left[ 0.1^2 + (\mu^{(n)})^2 - 2 w D (\mu^{(n)})^2 +w^2 (0.01^2 D + D (\mu^{(n)})^2)  \right]
% \]
% If we discard the "small" terms, we get the following.
% \[
%     0 = \sum_{n=1}^N\mathbb{E}\left[ \left(1 - 2 w D +w^2 D \right) (\mu^{(n)})^2  \right]
% \]
% We've removed the link between \(n\) and \(w\), so we can just solve the quadratic term, which gives.

% \newpage
% \begin{thebibliography}{1}
%
% \bibitem{williams}
%    Williams, David.
%    \textit{Probability with Martingales}.
%    Cambridge University Press, 1991.
%    Print.
%
% % Uncomment the following lines to include a webpage
% % \bibitem{webpage1}
% %   LastName, FirstName. ``Webpage Title''.
% %   WebsiteName, OrganizationName.
% %   Online; accessed Month Date, Year.\\
% %   \texttt{www.URLhere.com}
%
% \end{thebibliography}
% --- Document ends here ---

\end{document}
